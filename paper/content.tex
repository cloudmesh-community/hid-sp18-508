% status: 0
% chapter: PaaS

\title{Hadoop}



\author{Yue Guo}
\affiliation{%
  \institution{Indiana University}
  \streetaddress{Smith Research Center}
  \city{Bloomington} 
  \state{IN} 
  \postcode{47408}
  \country{USA}}
\email{yueguo@iu.edu}

\begin{abstract}
This paper provides a tutorial for Hadoop. Introduce what is Hadoop, how to use
 Hadoop and why using Hadoop.
\end{abstract}

\keywords{Hadoop, MapReduce, HDFS}


\maketitle

\section{Organization}
The document is to introduce Hadoop from three levels, the introduction of Hadoop
 and its  two main parts: MapReduce and HDFS. Then, show the application and 
 features about Hadoop, which can tell the advantage of Hadoop. And Finally 
 introduce the usecase of Hadoop and the environment setup.

\section{Introduction}
Nowadays, there are huge amounts of data in the world. And there are also lots of
 different models to analyze those data. No matter how good the models are, big 
 data can always bring better result. However, there is also some problems about
  storing big data and analyzing them. Disk capacity grows much faster than the 
  speed of Disk read. Hadoop can solve the problem. Hadoop is a distributed 
  computing open source framework of the Apache open source organization, It has 
  been used on many large websites such as Amazon, Facebook and Yahoo. The most 
  important part of Hadoop framework is: MapReduce and HDFS. HDFS is the abbreviation 
  of Hadoop Distributed File System and provides base-level support for distributed 
  computing storage. MapReduce is used to analyze and process data.

\section{MapReduce}
MapReduce can be splited to two verbs Map and Reduce. Map is to break down a task 
into multiple tasks, Reduce is to sum the results of multiple tasks and come up 
with the final result. 

``MapReduce program executes in three stages, namely map stage, shuffle stage, 
and reduce stage.

Map stage : The map or mapper?s job is to process the input data. Generally the 
input data is in the form of file or directory and is stored in the Hadoop file 
system (HDFS). The input file is passed to the mapper function line by line. The 
mapper processes the data and creates several small chunks of data.

Reduce stage : This stage is the combination of the Shuffle stage and the Reduce 
stage. The Reducer?s job is to process the data that comes from the mapper. After 
processing, it produces a new set of output, which will be stored in the HDFS''~\cite{hid-sp18-508-mapreduce}.
A architecture of Hadoop shown in Figure~\ref{f:fly}.

\begin{figure}[!ht]
  \centering\includegraphics[width=\columnwidth]{images/mapReduce.jpg}
  \caption{MapReduce Architecture}\label{f:fly}
\end{figure}

\section{HDFS}
When the amount of data exceed the ability of independent computer storage, it 
has to be splited and stored in more than one computer. HDFS is the storage 
foundation for distributed computing. HDFS is the abbreviation of Hadoop 
Distributed File System and the core of Hadoop. 

Hadoop's distributed file system is very similar with other distributed file 
systems. The basic characteristics of a distributed file system are: 

Extensibility.
Data Consistency.
Multiple servers work together to complete tasks that a single server cannot handle.
Multiple nodes in a distributed system can concurrently operate on shared resources.
Suitable for write once and read multiple times models.

\begin{figure}[!ht]
  \centering\includegraphics[width=\columnwidth]{images/HDFS.jpg}
  \caption{HDFS Architecture}\label{f:hdfs}
\end{figure}


The above figure~\ref{f:hdfs} shows the three important roles of HDFS: NameNode, 
DataNode, and Client. DataNode is the basic unit of file storage. Each file block 
can have multiple copies and be stored on different Datanodes. DataNode stores the 
Block in the local file system and sends all existing Block information to the 
NameNode. Client gets the access to data through interactions with the NameNode and 
DataNodes. Client contacts NameNode to get the Metadata of the file. The NameNode 
stores the Meta-data of the file system in memory. This information mainly includes 
the file information, and the information of each file block on the DataNode. 

The advantage of HDFS:

1. Suitable for batch processsing
2. Handle huge files: Can store petabytes of data on HDFS.
3. Access stream data: Write once and Read multiple times.
4. Cheap: the filesystem relies on commodity storage disks that are much cheaper 
than those used for enterprise grade storage. Besides, HDFS is open source and does 
not need pay for licensing fee.~\cite{hid-sp18-508-hdfs}.

The disadvantage of HDFS:
 Does not support multi-usr writes: HDFS is immutable, which means files cannot 
 be modified. One file can only be written once. 

\section{Hadoop}
\begin{figure}[!ht]
  \centering\includegraphics[width=\columnwidth]{images/hadoop.jpg}
  \caption{Hadoop Architecture }\label{f:hadoop}
\end{figure}


The above figure~\ref{f:hadoop} shows in the Hadoop system, there will be a Master, 
responsible for the work of NameNode and JobTracker. JobTracker's main responsibility 
is to start, track and schedule the execution of each Slave task. There will be more 
than one Slave, each Slave usually has the function of DataNode and is responsible 
for the TaskTracker's work. TaskTracker performs Map tasks and Reduce tasks in 
conjunction with local data based on application requirements.

\section{Application}
The most suitable is the analysis of massive data. In fact, Google first proposed 
MapReduce for mass data analysis. At the same time, HDFS was originally developed 
for search engine implementation and was later used in distributed computing 
frameworks. Mass data is divided into multiple nodes and then calculated by each 
node in parallel, and the results obtained are merged into the output. At the same 
time, the output of the first stage can be used as the input for the next stage 
of calculation. Therefore, it can be imagined that a tree-structured distributed 
computing map has different outputs at different stages. Simultaneous parallel 
and serial calculations can also be very well handled efficiently in distributed 
cluster resources.

\section{Features}
Extensiblity: Hadoop's distributed storage and distributed computing is done on 
cluster nodes, which also determines Hadoop can scale to more cluster nodes.

Economy: ``Hadoop generates cost benefits by bringing massively parallel computing 
to commodity servers, resulting in a substantial reduction in the cost per terabyte 
of storage, which in turn makes it reasonable to model all your data"~\cite{hid-sp18-508-features}

Reliablity: Through the distributed storage, Hadoop can automatically store multiple 
copies. When the data processing request fails, it will automatically redeploy the 
calculation task.

Efficience: The efficient data exchange implementation of the distributed file system 
and the combination of MapReduce and Local Data processing make basic preparations 
for efficient processing of large amounts of information.


\section{Environment Setup}
``Before installing Hadoop into the Linux environment,  Linux using ssh (Secure Shell) 
is needed. At the beginning, it is recommended to create a separate user for Hadoop 
to isolate Hadoop file system from Unix file system. SSH setup is required to do 
different operations on a cluster such as starting, stopping, distributed daemon 
shell operations. To authenticate different users of Hadoop, it is required to 
provide public/private key pair for a Hadoop user and share it with different 
users. Java is the main prerequisite for Hadoop. Download and extract Hadoop 2.4.1 
from Apache software foundation. 

Once you have downloaded Hadoop, Hadoop cluster can be operated in one of the 
three supported modes:

Local/Standalone Mode : After downloading Hadoop in your system, by default, 
it is configured in a standalone mode and can be run as a single java process.

Pseudo Distributed Mode : It is a distributed simulation on single machine. 
Each Hadoop daemon such as hdfs, yarn, MapReduce etc., will run as a separate 
java process. This mode is useful for development.

Fully Distributed Mode : This mode is fully distributed with minimum two or more 
machines as a cluster. We will come across this mode in detail in the coming 
chapters'''~\cite{hid-sp18-508-hadoop}. 


\section{Usecase}
Yahoo is the biggest proponent of Hadoop. Since Hadoop is created more than 10 
years ago, it has already become Yahoo's most important technology. At first, 
Hadoop is just used as Web searching, but with the times going, Yahoo's Hadoop 
application includes more aspects, which includes: advertising system, user 
behavior analysis, anti-spam system, personalized recommendation. Today, there 
are nearly 300 unique Hadoop platform application scenarios in different 
areas ~\cite{hid-sp18-508-yahoo}.

``Facebook runs the world?s largest Hadoop cluster" says Jay Parikh, Vice President 
Infrastructure Engineering, Facebook.As a world-renowned social networking site, has 
more than 300 million active users, of which approximately 30 million users update 
their status at least once a day; users upload more than 1 billion photos and 10 
million videos per month; and Weeks share 1 billion items, including logs, links, 
news, Weibo, etc. Therefore, the amount of data that Facebook needs to store and 
process is enormous. The cloud platform for performance is very important to Facebook, 
and Facebook mainly uses the Hadoop platform for ``searching, log processing, 
recommendation system, data warehousing and video and image analysis. Basically, 
Facebook runs the biggest Hadoop cluster that goes beyond 4,000 machines and storing 
more than hundreds of millions of gigabytes"~\cite{hid-sp18-508-fb}. Its first 
user-facing application, Facebook Messenger is also based on Hadoop database.

The world's largest supermarket chain, Wal-Mart, uses Hadoop to analyze the behavior 
of customers searching for products, and users search for keywords on the Wal-Mart 
website through search engines. Using the analysis results of these keywords to 
explore customer needs, in order to plan the next quarter's product promotion strategy. 
It even intends to analyze customers' discussion of products on social networking 
sites such as Facebook and Twitter, hoping to find customers? needs one step ahead 
of the competition.




\section{Conclusion}
Hadoop mainly consists of two parts: 1. HDFS, responsible for storage, distributed 
file system; 2. MapReduce, parallel processing framework, used to achieve task 
decomposition and scheduling. 

There are two main types of nodes in hadoop:

NameNode: management node, used to store file metadata, including a mapping table 
of files and data blocks; a mapping table of data blocks and data nodes.

DataNode: Work node, used to store the real data block.

And there are many applications on Hadoop.

\begin{acks}

  The authors would like to thank Dr.~Gregor~von~Laszewski for his
  support and suggestions to write this paper.

\end{acks}

\bibliographystyle{ACM-Reference-Format}
\bibliography{report} 

